{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the \"example\" module.\n",
    "\n",
    "The example module supplies one function, factorial().  For example,\n",
    "\n",
    ">>> factorial(5)\n",
    "120\n",
    "\"\"\"\n",
    "\n",
    "def factorial(n):\n",
    "    \"\"\"Return the factorial of n, an exact integer >= 0.\n",
    "\n",
    "    >>> [factorial(n) for n in range(6)]\n",
    "    [1, 1, 2, 6, 24, 120]\n",
    "    >>> factorial(30)\n",
    "    265252859812191058636308480000000\n",
    "    >>> factorial(-1)\n",
    "    Traceback (most recent call last):\n",
    "        ...\n",
    "    ValueError: n must be >= 0\n",
    "\n",
    "    Factorials of floats are OK, but the float must be an exact integer:\n",
    "    >>> factorial(30.1)\n",
    "    Traceback (most recent call last):\n",
    "        ...\n",
    "    ValueError: n must be exact integer\n",
    "    >>> factorial(30.0)\n",
    "    265252859812191058636308480000000\n",
    "\n",
    "    It must also not be ridiculously large:\n",
    "    >>> factorial(1e100)\n",
    "    Traceback (most recent call last):\n",
    "        ...\n",
    "    OverflowError: n too large\n",
    "    \"\"\"\n",
    "\n",
    "    import math\n",
    "    if not n >= 0:\n",
    "        raise ValueError(\"n must be >= 0\")\n",
    "    if math.floor(n) != n:\n",
    "        raise ValueError(\"n must be exact integer\")\n",
    "    if n+1 == n:  # catch a value like 1e300\n",
    "        raise OverflowError(\"n too large\")\n",
    "    result = 1\n",
    "    factor = 2\n",
    "    while factor <= n:\n",
    "        result *= factor\n",
    "        factor += 1\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when running a script pass -v to the script, and doctest prints a detailed log of what it’s trying, and prints a summary at the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another simple application of doctest is testing interactive examples in a text file. This can be done with the testfile() \n",
    "#function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any expected output must immediately follow the final '>>> ' or '... ' line containing the code, and the expected output (if any) extends to the next '>>> ' or all-whitespace line.\n",
    "\n",
    "The fine print:\n",
    "\n",
    "Expected output cannot contain an all-whitespace line, since such a line is taken to signal the end of expected output. If expected output does contain a blank line, put <BLANKLINE> in your doctest example each place a blank line is expected.\n",
    "\n",
    "All hard tab characters are expanded to spaces, using 8-column tab stops. Tabs in output generated by the tested code are not modified. Because any hard tabs in the sample output are expanded, this means that if the code output includes hard tabs, the only way the doctest can pass is if the NORMALIZE_WHITESPACE option or directive is in effect. Alternatively, the test can be rewritten to capture the output and compare it to an expected value as part of the test. This handling of tabs in the source was arrived at through trial and error, and has proven to be the least error prone way of handling them. It is possible to use a different algorithm for handling tabs by writing a custom DocTestParser class.\n",
    "\n",
    "Output to stdout is captured, but not output to stderr (exception tracebacks are captured via a different means).\n",
    "\n",
    "If you continue a line via backslashing in an interactive session, or for any other reason use a backslash, you should use a raw docstring, which will preserve your backslashes exactly as you type them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your collection of doctest’ed modules grows, you’ll want a way to run all their doctests systematically. doctest provides two functions that can be used to create unittest test suites from modules and text files containing doctests. To integrate with unittest test discovery, include a load_tests() function in your test module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some of my examples\n",
    "\n",
    "def add(a, b):\n",
    "    '''\n",
    "    This is a test:\n",
    "    >>> add(2, 2)\n",
    "    5\n",
    "    '''\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    factorial(5)\n",
      "Expecting:\n",
      "    120\n",
      "ok\n",
      "Trying:\n",
      "    add(2, 2)\n",
      "Expecting:\n",
      "    5\n",
      "**********************************************************************\n",
      "File \"__main__\", line 6, in __main__.add\n",
      "Failed example:\n",
      "    add(2, 2)\n",
      "Expected:\n",
      "    5\n",
      "Got:\n",
      "    4\n",
      "Trying:\n",
      "    [factorial(n) for n in range(6)]\n",
      "Expecting:\n",
      "    [1, 1, 2, 6, 24, 120]\n",
      "ok\n",
      "Trying:\n",
      "    factorial(30)\n",
      "Expecting:\n",
      "    265252859812191058636308480000000\n",
      "ok\n",
      "Trying:\n",
      "    factorial(-1)\n",
      "Expecting:\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    ValueError: n must be >= 0\n",
      "ok\n",
      "Trying:\n",
      "    factorial(30.1)\n",
      "Expecting:\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    ValueError: n must be exact integer\n",
      "ok\n",
      "Trying:\n",
      "    factorial(30.0)\n",
      "Expecting:\n",
      "    265252859812191058636308480000000\n",
      "ok\n",
      "Trying:\n",
      "    factorial(1e100)\n",
      "Expecting:\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    OverflowError: n too large\n",
      "ok\n",
      "2 items passed all tests:\n",
      "   1 tests in __main__\n",
      "   6 tests in __main__.factorial\n",
      "**********************************************************************\n",
      "1 items had failures:\n",
      "   1 of   1 in __main__.add\n",
      "8 tests in 3 items.\n",
      "7 passed and 1 failed.\n",
      "***Test Failed*** 1 failures.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=1, attempted=8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import doctest\n",
    "doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNITTEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_add (__main__.TestNotebook) ... FAIL\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_add (__main__.TestNotebook)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-16-3c7fcaf6ec27>\", line 6, in test_add\n",
      "    self.assertEqual(add(2, 2), 5)\n",
      "AssertionError: 4 != 5\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x2b87b77b220>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "    \n",
    "    def test_add(self):\n",
    "        self.assertEqual(add(2, 2), 5)\n",
    "        \n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also made .py and .txt files in order to run the programs outside of jupyter notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
